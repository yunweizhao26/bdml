{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gh1SrzRp9LIV"
   },
   "source": [
    "# HPC Tutorial\n",
    "\n",
    "Welcome to the **HPC (High-Performance Computing) Tutorial**. This guide will help you understand how to access NYU's Cloud Burst HPC cluster, manage your files, run interactive and batch jobs, set up environments, run jupyter notebooks and more.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QKzC6deN9Dsj"
   },
   "source": [
    "## Table of Contents\n",
    "\n",
    "1. [Logging In](#logging-in)\n",
    "2. [Understanding the Filesystem](#understanding-the-filesystem)\n",
    "3. [Running Interactive Jobs](#running-interactive-jobs)\n",
    "4. [Setting Up Singularity and Conda](#setting-up-singularity-and-conda)\n",
    "5. [Running Batch Jobs](#running-batch-jobs)\n",
    "6. [SCP for copying files around](#copy-files)\n",
    "7. [Running Jupyter notebooks](#jupyter)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "srun --account=csci_ga_3033_077-2025sp --partition=n1s8-v100-1 --gres=gpu:1 --time=1:00:00 --pty /bin/bash\n",
    "srun --account=csci_ga_3033_077-2025sp --partition=interactive --time=1:00:00 --pty /bin/bash\n",
    "cd /scratch/yz5944\n",
    "singularity exec --bind /scratch --nv --overlay  /scratch/yz5944/overlay-25GB-500K.ext3:rw /scratch/yz5944/cuda11.8.86-cudnn8.7-devel-ubuntu22.04.2.sif /bin/bash\n",
    "\n",
    "ps -f -u $USER | grep Singularity\n",
    "kill -9 <PID>\n",
    "\n",
    "source /ext3/env.sh\n",
    "conda activate bdml_env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KkG_1WrS9XA6"
   },
   "source": [
    "## Logging In\n",
    "\n",
    "To access the Greene HPC cluster, you need to be on the NYU network. If you're off-campus, connect via the [NYU VPN](https://www.nyu.edu/life/information-technology/infrastructure/network-services/vpn.html).\n",
    "\n",
    "### Steps to Log In\n",
    "\n",
    "1. **Open a Terminal** on your local machine.\n",
    "\n",
    "2. **Connect via SSH** (replace `yz5944` with your NYU NetID):\n",
    "\n",
    "```bash\n",
    "Local ---> Greene login node ---> Greene compute node (NOT USING FOR THIS COURSE)\n",
    "                            |\n",
    "                             ---> Burst node    ---> GCP compute node\n",
    "```\n",
    "\n",
    "```bash\n",
    "ssh yz5944@greene.hpc.nyu.edu\n",
    "ssh burst\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aFHXJsbO_lAI"
   },
   "source": [
    "\n",
    "## Understanding the Filesystem\n",
    "\n",
    "The Greene HPC cluster has different directories optimized for various storage needs.\n",
    "\n",
    "| Directory  | Variable   | Purpose                | Flushed After | Quota             |\n",
    "|------------|------------|------------------------|---------------|-------------------|\n",
    "| `/archive` | `$ARCHIVE` | Long-term storage      | No            | 2TB / 20K inodes  |\n",
    "| `/home`    | `$HOME`    | Configuration files    | No            | 50GB / 30K inodes |\n",
    "| `/scratch` | `$SCRATCH` | Temporary data storage | Yes (60 days) | 5TB / 1M inodes   |\n",
    "\n",
    "\n",
    "- **Check Your Quota:**\n",
    "\n",
    "  ```bash\n",
    "  myquota\n",
    "  ```\n",
    "\n",
    "- **Recommended:** Store the data you want to keep in `/scratch/yz5944` and temporary data in `/tmp`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LScixmBRA8KE"
   },
   "source": [
    "\n",
    "## Running Interactive Jobs\n",
    "\n",
    "When you need to run scripts or perform debugging interactively, follow the steps below.\n",
    "\n",
    "### Typical Workflow\n",
    "\n",
    "1. Log in: Greene’s login node.\n",
    "2. Log in to Burst node.\n",
    "3. Request a job / computational resource and wait until Slurm grants it.\n",
    "  - You always need to request a job for GPUs.\n",
    "4. Execute singularity and start container instance.\n",
    "5. Activate conda environment with your own deep learning libraries.\n",
    "6. Run your code, make changes/debugging.\n",
    "\n",
    "### Accounts and Partitions\n",
    "\n",
    "- **Account:** `csci_ga_3033_077-2025sp`\n",
    "\n",
    "- **Partitions:**\n",
    "  - `interactive` (for lightweight tasks)\n",
    "  - `n1s8-v100-1` (for GPU tasks)\n",
    "  - `n1s16-v100-2`\n",
    "  - `n2c48m24`\n",
    "  - `g2-standard-12`\n",
    "  - `g2-standard-24`\n",
    "  - `c12m85-a100-1`\n",
    "  - `c24m170-a100-2`\n",
    "\n",
    "#### Understanding Partitions\n",
    "\n",
    "- **Partitions** are specific resources or queues on the cluster.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BIJTuXclC0jt"
   },
   "source": [
    "### Simple Scripts and File Operations\n",
    "\n",
    "For non-GPU tasks, use the `interactive` partition.\n",
    "\n",
    "**Requesting an Interactive Session:**\n",
    "\n",
    "```bash\n",
    "srun --account=csci_ga_3033_077-2025sp --partition=interactive --pty /bin/bash\n",
    "```\n",
    "\n",
    "- **Options:**\n",
    "  - `--account`: Specify account.\n",
    "  - `--partition`: Choose partition.\n",
    "  - `--pty /bin/bash`: Open an interactive shell.\n",
    "\n",
    "> **Tip:** After allocation, verify the node:\n",
    "\n",
    "```bash\n",
    "hostname\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### GPU Access\n",
    "\n",
    "For GPU tasks, request resources from a GPU partition. Each student is assigned to Slurm account with 200 GPU hours (12000 minutes) and sufficient CPU time.\n",
    "\n",
    "**Requesting a GPU Session:**\n",
    "\n",
    "```bash\n",
    "srun --account=csci_ga_3033_077-2025sp --partition=n1s8-v100-1 --gres=gpu:1 --time=1:00:00 --pty /bin/bash\n",
    "```\n",
    "\n",
    "- **Options:**\n",
    "  - `--gres=gpu:1`: Request one GPU.\n",
    "  - `--time=1:00:00`: Set time limit.\n",
    "\n",
    "**Verify GPU Allocation:**\n",
    "\n",
    "```bash\n",
    "nvidia-smi\n",
    "```\n",
    "---\n",
    "\n",
    "### Monitoring Jobs\n",
    "\n",
    "Check the status of your jobs in the Slurm queue.\n",
    "\n",
    "**Check Your Jobs:**\n",
    "\n",
    "```bash\n",
    "squeue -u yz5944\n",
    "```\n",
    "\n",
    "**Cancel a Job:**\n",
    "\n",
    "- **Exit the Session:** Press `Ctrl+D` or type `exit`.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t0om9zARoB2v"
   },
   "source": [
    "## Setting Up Singularity and Conda\n",
    "\n",
    "### Copying the Filesystem Image\n",
    "\n",
    "Copy the empty filesystem image (once per semester).\n",
    "\n",
    "**Get on a GPU Node:**\n",
    "\n",
    "```bash\n",
    "srun --account=csci_ga_3033_077-2025sp --cpus-per-task=2 --mem=16GB --partition=n1s8-v100-1 --gres=gpu:v100:1 --time=04:00:00 --pty /bin/bash\n",
    "```\n",
    "\n",
    "**Navigate to Scratch Directory:**\n",
    "\n",
    "```bash\n",
    "cd /scratch/yz5944\n",
    "```\n",
    "\n",
    "**Download Overlay Filesystem:**\n",
    "\n",
    "```bash\n",
    "scp greene-dtn:/scratch/work/public/overlay-fs-ext3/overlay-25GB-500K.ext3.gz .\n",
    "```\n",
    "\n",
    "Filesystems can be mounted as read-write (`rw`) or read-only (`ro`) when we use it with singularity.\n",
    "- read-write: use this one when setting up env (installing conda, libs, other static files)\n",
    "- read-only: use this one when running your jobs. It has to be read-only since multiple processes will access the same image. It will crash if any job has already mounted it as read-write.\n",
    "\n",
    "### Unzipping the Image\n",
    "\n",
    "Unzip the ext3 filesystem (takes about 5 minutes).\n",
    "\n",
    "```bash\n",
    "gunzip -vvv ./overlay-25GB-500K.ext3.gz\n",
    "```\n",
    "\n",
    "```bash\n",
    "# Copy the appropriate singularity image to the current working directory\n",
    "scp -rp greene-dtn:/scratch/work/public/singularity/cuda11.8.86-cudnn8.7-devel-ubuntu22.04.2.sif .\n",
    "```\n",
    "\n",
    "\n",
    "### Installing Conda\n",
    "\n",
    "Install Conda inside the Singularity container.\n",
    "\n",
    "**Start Singularity:**\n",
    "\n",
    "```bash\n",
    "singularity exec --bind /scratch --nv --overlay  /scratch/yz5944/overlay-25GB-500K.ext3:rw /scratch/yz5944/cuda11.8.86-cudnn8.7-devel-ubuntu22.04.2.sif /bin/bash\n",
    "```\n",
    "\n",
    "**Inside Singularity:**\n",
    "\n",
    "Download and install conda\n",
    "```bash\n",
    "cd /ext3/\n",
    "wget --no-check-certificate https://github.com/conda-forge/miniforge/releases/latest/download/Miniforge3-Linux-x86_64.sh\n",
    "sh Miniforge3-Linux-x86_64.sh -b -p /ext3/miniforge3\n",
    "```\n",
    "\n",
    "Create wrapper script\n",
    "```bash\n",
    "touch /ext3/env.sh\n",
    "echo '#!/bin/bash' >> /ext3/env.sh\n",
    "echo 'unset -f which' >> /ext3/env.sh\n",
    "echo 'source /ext3/miniforge3/etc/profile.d/conda.sh' >> /ext3/env.sh\n",
    "echo 'export PATH=/ext3/miniforge3/bin:$PATH'         >> /ext3/env.sh\n",
    "echo 'export PYTHONPATH=/ext3/miniforge3/bin:$PATH'   >> /ext3/env.sh\n",
    "```\n",
    "Activate conda environment\n",
    "\n",
    "```bash\n",
    "source /ext3/env.sh\n",
    "```\n",
    "Update Conda and Install Packages\n",
    "```bash\n",
    "conda config --remove channels defaults\n",
    "conda update -n base conda -y\n",
    "conda clean --all --yes\n",
    "```\n",
    "\n",
    "\n",
    "```bash\n",
    "conda create -n bdml_env python==3.9\n",
    "conda activate bdml_env\n",
    "conda install pip --yes\n",
    "conda install ipykernel --yes\n",
    "conda install pytorch\n",
    "```\n",
    "\n",
    "### Testing the Setup\n",
    "\n",
    "Test PyTorch and GPU access:\n",
    "\n",
    "```python\n",
    "python\n",
    "\n",
    ">>> import torch\n",
    ">>> torch.cuda.is_available()\n",
    "True\n",
    ">>> x = torch.tensor([1, 2])\n",
    ">>> x\n",
    "tensor([1, 2])\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BHts73WJvTpW"
   },
   "source": [
    "\n",
    "## Running Batch Jobs\n",
    "\n",
    "For longer experiments or multiple jobs, use batch jobs.\n",
    "\n",
    "### Batch Job Workflow\n",
    "\n",
    "1. **Log In** to Greene.\n",
    "\n",
    "2. **Submit an `sbatch` Script**.\n",
    "\n",
    "### Submitting a Job Script\n",
    "\n",
    "```bash\n",
    "Request an interactive shell\n",
    "```\n",
    "\n",
    "**Write the Batch Script:**\n",
    "\n",
    "```bash\n",
    "#SBATCH --job-name=job_wgpu\n",
    "#SBATCH --account=csci_ga_3033_077-2025sp\n",
    "#SBATCH --partition=n1s8-v100-1\n",
    "#SBATCH --open-mode=append\n",
    "#SBATCH --output=./%j_%x.out\n",
    "#SBATCH --error=./%j_%x.err\n",
    "#SBATCH --export=ALL\n",
    "#SBATCH --time=00:10:00\n",
    "#SBATCH --gres=gpu:1\n",
    "#SBATCH --requeue\n",
    "\n",
    "\n",
    "singularity exec --bind /scratch --nv --overlay  /scratch/yz5944/overlay-25GB-500K.ext3:rw /scratch/yz5944/cuda11.8.86-cudnn8.7-devel-ubuntu22.04.2.sif /bin/bash -c \"\n",
    "source /ext3/env.sh\n",
    "conda activate bdml_env\n",
    "cd /scratch/yz5944/bdml/\n",
    "python ./test_script.py\n",
    "\"\n",
    "```\n",
    "\n",
    "**Submit Batch Job:**\n",
    "\n",
    "```bash\n",
    "sbatch gpu_job.slurm\n",
    "```\n",
    "\n",
    "**Check Job Status:**\n",
    "\n",
    "```bash\n",
    "squeue -u yz5944\n",
    "```\n",
    "\n",
    "### Checking Job Output\n",
    "\n",
    "After job completion, check the output log.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-xqFD7KveBLx"
   },
   "source": [
    "### SCP Tutorial How to copy files around?\n",
    "\n",
    "* Use Git and you don't need any of these :)\n",
    "\n",
    "* From local to Greene, on local run\n",
    "\n",
    "```bash\n",
    "scp [optional flags] [file-path] yz5944@greene.hpc.nyu.edu:[greene-destination-path]\n",
    "```\n",
    "\n",
    "* From Greene to local, on local run\n",
    "\n",
    "```bash\n",
    "scp [optional flags] yz5944@greene.hpc.nyu.edu:[file-path] [local-destination-path]\n",
    "```\n",
    "\n",
    "* From Greene to GCP, on GCP run\n",
    "\n",
    "```bash\n",
    "scp [optional flags] greene-dtn:[file-path] [gcp-destination-path]\n",
    "```\n",
    "\n",
    "* From GCP to Greene, on GCP run\n",
    "\n",
    "```bash\n",
    "scp [optional flags] [file-path] greene-dtn:[greene-destination-path]\n",
    "```\n",
    "\n",
    "* From local to GCP: local → Greene → GCP\n",
    "* From GCP to local: GCP → Greene → Local"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9HjD8Zitcbmk"
   },
   "source": [
    "### Running Jupyter Notebook\n",
    "\n",
    "Create jupyter kernel\n",
    "```Bash\n",
    "mkdir -p ~/.local/share/jupyter/kernels\n",
    "cd ~/.local/share/jupyter/kernels\n",
    "scp -r greene-dtn:/share/apps/mypy/src/kernel_template ./my_env\n",
    "cd ./my_env\n",
    "\n",
    "ls\n",
    "#kernel.json  logo-32x32.png  logo-64x64.png  python\n",
    "```\n",
    "In the 'python' file, change the singularity command at the bottom to\n",
    "```Bash\n",
    "singularity exec $nv \\\n",
    "  --overlay /scratch/yz5944/overlay-25GB-500K.ext3:ro \\\n",
    "  /scratch/yz5944/cuda11.8.86-cudnn8.7-devel-ubuntu22.04.2.sif \\\n",
    "  /bin/bash -c \"source /ext3/env.sh; conda activate bdml_env; $cmd $args\"\n",
    "```\n",
    "Edit the default kernel.json file by setting PYTHON_LOCATION and KERNEL_DISPLAY_NAME.\n",
    "```Bash\n",
    "{\n",
    " \"argv\": [\n",
    "  \"/home/yz5944/.local/share/jupyter/kernels/my_env/python\", #PYTHON_LOCATION\n",
    "  \"-m\",\n",
    "  \"ipykernel_launcher\",\n",
    "  \"-f\",\n",
    "  \"{connection_file}\"\n",
    " ],\n",
    " \"display_name\": \"my_env\", #KERNEL_DISPLAY_NAME\n",
    " \"language\": \"python\"\n",
    "}\n",
    "\n",
    "```\n",
    "Go to https://ood-burst-001.hpc.nyu.edu/ to run Jupyter Notebook and VS Code.\n",
    "\n",
    "Troubleshooting: https://sites.google.com/nyu.edu/nyu-hpc/training-support/general-hpc-topics/tunneling-and-x11-forwarding\n",
    "\n",
    "Acknowledgement: Thanks to Divyam Madan for providing the base version for this tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WPrH4uBjzy7B"
   },
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "1eel1nLYmu0jDU8ombia1e4gxD03tG6IW",
     "timestamp": 1738617664221
    },
    {
     "file_id": "1v0M4XwEPysR7_EnnyjMGAJlZBjYqqHWh",
     "timestamp": 1738600602895
    },
    {
     "file_id": "1Daef4tq1jYrmQlxPeo0UYoBpzZMderwx",
     "timestamp": 1726692141453
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
